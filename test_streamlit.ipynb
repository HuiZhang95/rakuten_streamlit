{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "absl-py                 2.1.0\n",
      "aiohappyeyeballs        2.4.3\n",
      "aiohttp                 3.10.5\n",
      "aiosignal               1.2.0\n",
      "appnope                 0.1.4\n",
      "asttokens               3.0.0\n",
      "astunparse              1.6.3\n",
      "attrs                   24.2.0\n",
      "blinker                 1.6.2\n",
      "Brotli                  1.0.9\n",
      "cachetools              5.3.3\n",
      "certifi                 2024.8.30\n",
      "cffi                    1.17.1\n",
      "charset-normalizer      3.3.2\n",
      "click                   8.1.7\n",
      "comm                    0.2.2\n",
      "contourpy               1.3.1\n",
      "cryptography            41.0.3\n",
      "cycler                  0.12.1\n",
      "debugpy                 1.8.10\n",
      "decorator               5.1.1\n",
      "exceptiongroup          1.2.2\n",
      "executing               2.1.0\n",
      "flatbuffers             24.3.25\n",
      "fonttools               4.55.3\n",
      "frozenlist              1.5.0\n",
      "gast                    0.4.0\n",
      "google-auth             2.29.0\n",
      "google-auth-oauthlib    0.5.2\n",
      "google-pasta            0.2.0\n",
      "grpcio                  1.48.2\n",
      "h5py                    3.12.1\n",
      "idna                    3.7\n",
      "imbalanced-learn        0.12.4\n",
      "imblearn                0.0\n",
      "importlib_metadata      8.5.0\n",
      "ipykernel               6.29.5\n",
      "ipython                 8.30.0\n",
      "jedi                    0.19.2\n",
      "joblib                  1.4.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.7.2\n",
      "keras                   2.12.0\n",
      "Keras-Preprocessing     1.1.2\n",
      "kiwisolver              1.4.7\n",
      "Markdown                3.4.1\n",
      "MarkupSafe              2.1.3\n",
      "matplotlib              3.9.3\n",
      "matplotlib-inline       0.1.7\n",
      "multidict               6.1.0\n",
      "nest_asyncio            1.6.0\n",
      "numpy                   1.23.5\n",
      "oauthlib                3.2.2\n",
      "opt-einsum              3.3.0\n",
      "packaging               24.2\n",
      "pandas                  1.5.2\n",
      "parso                   0.8.4\n",
      "pexpect                 4.9.0\n",
      "pickleshare             0.7.5\n",
      "pillow                  11.0.0\n",
      "pip                     24.3.1\n",
      "platformdirs            4.3.6\n",
      "prompt_toolkit          3.0.48\n",
      "propcache               0.2.0\n",
      "protobuf                3.20.3\n",
      "psutil                  6.1.0\n",
      "ptyprocess              0.7.0\n",
      "pure_eval               0.2.3\n",
      "pyasn1                  0.4.8\n",
      "pyasn1-modules          0.2.8\n",
      "pycparser               2.21\n",
      "Pygments                2.18.0\n",
      "PyJWT                   2.9.0\n",
      "pyOpenSSL               23.2.0\n",
      "pyparsing               3.2.0\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2024.2\n",
      "pyzmq                   26.2.0\n",
      "requests                2.32.3\n",
      "requests-oauthlib       2.0.0\n",
      "rsa                     4.7.2\n",
      "scikit-learn            1.6.0\n",
      "scipy                   1.14.1\n",
      "setuptools              75.6.0\n",
      "six                     1.17.0\n",
      "stack_data              0.6.3\n",
      "tensorboard             2.12.1\n",
      "tensorboard_data_server 0.7.0\n",
      "tensorboard-plugin-wit  1.6.0\n",
      "tensorflow              2.12.0\n",
      "tensorflow-estimator    2.12.0\n",
      "termcolor               2.1.0\n",
      "threadpoolctl           3.5.0\n",
      "tornado                 6.4.2\n",
      "traitlets               5.14.3\n",
      "typing_extensions       4.12.2\n",
      "urllib3                 2.2.3\n",
      "wcwidth                 0.2.13\n",
      "Werkzeug                3.0.6\n",
      "wheel                   0.35.1\n",
      "wrapt                   1.14.1\n",
      "yarl                    1.18.0\n",
      "zipp                    3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.5.2\n",
      "  Downloading pandas-1.5.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/hui/anaconda3/envs/tf/lib/python3.11/site-packages (from pandas==1.5.2) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas==1.5.2)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/hui/anaconda3/envs/tf/lib/python3.11/site-packages (from pandas==1.5.2) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hui/anaconda3/envs/tf/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas==1.5.2) (1.17.0)\n",
      "Downloading pandas-1.5.2-cp311-cp311-macosx_10_9_x86_64.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.5.2 pytz-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, RNN, GRUCell, Dropout\n",
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_rnn():\n",
    "    with open('images_hz/model_RNN.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "        _, model, _, _ = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_svc():\n",
    "    with open('images_hz/model_SVC.pkl','rb') as f:\n",
    "        model, _, _ = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "def loadModels(classifier):\n",
    "    if classifier == 'RNN':\n",
    "        clf = load_rnn()\n",
    "    elif classifier == 'SVC':\n",
    "        clf = load_svc()\n",
    "    return clf\n",
    "\n",
    "def scores(clf, user_input_word):\n",
    "    return clf.predict(user_input_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from cleantext import clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {'a','about','above','after','again','against','ain','all','am','an','and','any','are','aren',\"aren't\",'as','at',\n",
    "'be','because','been','before','being','below','between','both','but','by','can','couldn',\"couldn't\",'d','did','didn',\"didn't\",\n",
    "'do','does','doesn',\"doesn't\",'doing','don',\"don't\",'down','during','each','few','for','from','further','had','hadn',\"hadn't\",\n",
    "'has','hasn',\"hasn't\",'have','haven',\"haven't\",'having','he','her','here','hers','herself','him','himself','his','how','i','if',\n",
    "'in','into','is','isn',\"isn't\",'it',\"it's\",'its','itself','just','ll','m','ma','me','mightn',\"mightn't\",'more','most','mustn',\"mustn't\",\n",
    "'my','myself','needn',\"needn't\",'no','nor','not','now','o','of','off','on','once','only','or','other','our','ours','ourselves','out',\n",
    "'over','own','re','s','same','shan',\"shan't\",'she',\"she's\",'should',\"should've\",'shouldn',\"shouldn't\",'so','some','such','t','than',\n",
    "'that',\"that'll\",'the','their','theirs','them','themselves','then','there','these','they','this','those','through','to','too',\n",
    "'under','until','up','ve','very','was','wasn',\"wasn't\",'we','were','weren',\"weren't\",'what','when','where','which','while','who',\n",
    "'whom','why','will','with','won',\"won't\",'wouldn',\"wouldn't\",'y','you',\"you'd\",\"you'll\",\"you're\",\"you've\",'your','yours','yourself',\n",
    "'yourselves'}\n",
    "stop_words.update(['the', 'and','for', 'from', 'was', 'what', 'with', 'this', \\\n",
    "                'that',  'don', 'pure', 'lot', 'are', 'who', 'more', 'will', 'tab', \\\n",
    "                'each' , 'would', 'but', 'not','its','all','your', 'last','over', \\\n",
    "                    'are','you', 'can', 'above', 'his','she','ready', 'yes', \\\n",
    "                    'size'])\n",
    "stop_words.remove(\"no\")\n",
    "stemmer = EnglishStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(words) :\n",
    "    output = []\n",
    "    for string in words :\n",
    "        lemma = wordnet_lemmatizer.lemmatize(string)\n",
    "        if (lemma not in output) : output.append(lemma)\n",
    "    return output\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = clean(\n",
    "        text = text,\n",
    "        fix_unicode = True,\n",
    "        to_ascii=True,\n",
    "        lower = True,\n",
    "        replace_with_url = ' ',\n",
    "        replace_with_email = ' ',\n",
    "        lang = 'en'\n",
    "    )\n",
    "    # Remove all special characters and punctuation\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]+\", \" \", text)\n",
    "    # Remove repeated characters\n",
    "    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n",
    "    # remove extra spaces, tabs, and new lines\n",
    "    text = \" \".join(text.split())\n",
    "    w = str(text)\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z0-9?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "    # stem the word\n",
    "    words = word_tokenize(w.strip())\n",
    "    words2 = [stemmer.stem(word) for word in words]\n",
    "    # lemmatize the word\n",
    "    words3 = lemmatization(words2)\n",
    "\n",
    "    # remove stopword\n",
    "    words = [word for word in words3 if word not in stop_words]\n",
    "\n",
    "    if len(words) < 1: # sometimes, all words are removed\n",
    "        return w\n",
    "    else:\n",
    "        return ' '.join(words).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hui/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/hui/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator GridSearchCV from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Merry Christmas!'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m clf \u001b[38;5;241m=\u001b[39m loadModels(option)\n\u001b[1;32m      4\u001b[0m user_input_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerry Christmas!\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m output_st \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input_word\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mscores\u001b[0;34m(clf, user_input_word)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscores\u001b[39m(clf, user_input_word):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input_word\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/model_selection/_search.py:597\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    596\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/svm/_base.py:821\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    819\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 821\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/svm/_base.py:435\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    420\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/svm/_base.py:613\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[0;32m--> 613\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    624\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/utils/validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/sklearn/utils/_array_api.py:832\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    830\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 832\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Merry Christmas!'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# use the st.selectbox() method to choose between the RandomForest classifier, the SVM classifier and the LogisticRegression classifier. Then return to the Streamlit web application to view the select box.\n",
    "option = 'SVC'\n",
    "clf = loadModels(option)\n",
    "user_input_word = 'Merry Christmas!'\n",
    "output_st = scores(clf, user_input_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
